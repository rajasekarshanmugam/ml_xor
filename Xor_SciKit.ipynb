{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16166fb3-2a5c-487d-bac2-30cd1dbdd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"np:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1022aa4-8a4f-419f-a61e-fefffeb2da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([ [0., 0.], [0., 1.], [1., 0.], [1., 1.] ])\n",
    "y = np.array([ 0., 1., 1., 0. ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79cc1b6c-e164-42b7-885b-06b4277f9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70813602\n",
      "Iteration 2, loss = 0.70761905\n",
      "Iteration 3, loss = 0.70711055\n",
      "Iteration 4, loss = 0.70661062\n",
      "Iteration 5, loss = 0.70611937\n",
      "Iteration 6, loss = 0.70563681\n",
      "Iteration 7, loss = 0.70516290\n",
      "Iteration 8, loss = 0.70469746\n",
      "Iteration 9, loss = 0.70424023\n",
      "Iteration 10, loss = 0.70379082\n",
      "Iteration 11, loss = 0.70334884\n",
      "Iteration 12, loss = 0.70291390\n",
      "Iteration 13, loss = 0.70248573\n",
      "Iteration 14, loss = 0.70206408\n",
      "Iteration 15, loss = 0.70164878\n",
      "Iteration 16, loss = 0.70123965\n",
      "Iteration 17, loss = 0.70083653\n",
      "Iteration 18, loss = 0.70043926\n",
      "Iteration 19, loss = 0.70004766\n",
      "Iteration 20, loss = 0.69966152\n",
      "Iteration 21, loss = 0.69928066\n",
      "Iteration 22, loss = 0.69890485\n",
      "Iteration 23, loss = 0.69853387\n",
      "Iteration 24, loss = 0.69816748\n",
      "Iteration 25, loss = 0.69780542\n",
      "Iteration 26, loss = 0.69744745\n",
      "Iteration 27, loss = 0.69709331\n",
      "Iteration 28, loss = 0.69674276\n",
      "Iteration 29, loss = 0.69639553\n",
      "Iteration 30, loss = 0.69605139\n",
      "Iteration 31, loss = 0.69571010\n",
      "Iteration 32, loss = 0.69537143\n",
      "Iteration 33, loss = 0.69503518\n",
      "Iteration 34, loss = 0.69470114\n",
      "Iteration 35, loss = 0.69436914\n",
      "Iteration 36, loss = 0.69403900\n",
      "Iteration 37, loss = 0.69371057\n",
      "Iteration 38, loss = 0.69338372\n",
      "Iteration 39, loss = 0.69305831\n",
      "Iteration 40, loss = 0.69273425\n",
      "Iteration 41, loss = 0.69241142\n",
      "Iteration 42, loss = 0.69208976\n",
      "Iteration 43, loss = 0.69176917\n",
      "Iteration 44, loss = 0.69144960\n",
      "Iteration 45, loss = 0.69113098\n",
      "Iteration 46, loss = 0.69081328\n",
      "Iteration 47, loss = 0.69049643\n",
      "Iteration 48, loss = 0.69018041\n",
      "Iteration 49, loss = 0.68986519\n",
      "Iteration 50, loss = 0.68955073\n",
      "Iteration 51, loss = 0.68923702\n",
      "Iteration 52, loss = 0.68892402\n",
      "Iteration 53, loss = 0.68861173\n",
      "Iteration 54, loss = 0.68830013\n",
      "Iteration 55, loss = 0.68798920\n",
      "Iteration 56, loss = 0.68767894\n",
      "Iteration 57, loss = 0.68736932\n",
      "Iteration 58, loss = 0.68706034\n",
      "Iteration 59, loss = 0.68675200\n",
      "Iteration 60, loss = 0.68644428\n",
      "Iteration 61, loss = 0.68613716\n",
      "Iteration 62, loss = 0.68583066\n",
      "Iteration 63, loss = 0.68552475\n",
      "Iteration 64, loss = 0.68521942\n",
      "Iteration 65, loss = 0.68491468\n",
      "Iteration 66, loss = 0.68461050\n",
      "Iteration 67, loss = 0.68430689\n",
      "Iteration 68, loss = 0.68400382\n",
      "Iteration 69, loss = 0.68370130\n",
      "Iteration 70, loss = 0.68339932\n",
      "Iteration 71, loss = 0.68309785\n",
      "Iteration 72, loss = 0.68279690\n",
      "Iteration 73, loss = 0.68249645\n",
      "Iteration 74, loss = 0.68219650\n",
      "Iteration 75, loss = 0.68189702\n",
      "Iteration 76, loss = 0.68159802\n",
      "Iteration 77, loss = 0.68129947\n",
      "Iteration 78, loss = 0.68100137\n",
      "Iteration 79, loss = 0.68070371\n",
      "Iteration 80, loss = 0.68040648\n",
      "Iteration 81, loss = 0.68010966\n",
      "Iteration 82, loss = 0.67981324\n",
      "Iteration 83, loss = 0.67951721\n",
      "Iteration 84, loss = 0.67922156\n",
      "Iteration 85, loss = 0.67892628\n",
      "Iteration 86, loss = 0.67863136\n",
      "Iteration 87, loss = 0.67833677\n",
      "Iteration 88, loss = 0.67804252\n",
      "Iteration 89, loss = 0.67774860\n",
      "Iteration 90, loss = 0.67745498\n",
      "Iteration 91, loss = 0.67716165\n",
      "Iteration 92, loss = 0.67686862\n",
      "Iteration 93, loss = 0.67657585\n",
      "Iteration 94, loss = 0.67628335\n",
      "Iteration 95, loss = 0.67599110\n",
      "Iteration 96, loss = 0.67569909\n",
      "Iteration 97, loss = 0.67540731\n",
      "Iteration 98, loss = 0.67511574\n",
      "Iteration 99, loss = 0.67482438\n",
      "Iteration 100, loss = 0.67453321\n",
      "Iteration 101, loss = 0.67424222\n",
      "Iteration 102, loss = 0.67395141\n",
      "Iteration 103, loss = 0.67366075\n",
      "Iteration 104, loss = 0.67337024\n",
      "Iteration 105, loss = 0.67307986\n",
      "Iteration 106, loss = 0.67278961\n",
      "Iteration 107, loss = 0.67249947\n",
      "Iteration 108, loss = 0.67220944\n",
      "Iteration 109, loss = 0.67191949\n",
      "Iteration 110, loss = 0.67162962\n",
      "Iteration 111, loss = 0.67133982\n",
      "Iteration 112, loss = 0.67105008\n",
      "Iteration 113, loss = 0.67076038\n",
      "Iteration 114, loss = 0.67047071\n",
      "Iteration 115, loss = 0.67018106\n",
      "Iteration 116, loss = 0.66989143\n",
      "Iteration 117, loss = 0.66960179\n",
      "Iteration 118, loss = 0.66931214\n",
      "Iteration 119, loss = 0.66902247\n",
      "Iteration 120, loss = 0.66873277\n",
      "Iteration 121, loss = 0.66844301\n",
      "Iteration 122, loss = 0.66815320\n",
      "Iteration 123, loss = 0.66786332\n",
      "Iteration 124, loss = 0.66757336\n",
      "Iteration 125, loss = 0.66728330\n",
      "Iteration 126, loss = 0.66699315\n",
      "Iteration 127, loss = 0.66670288\n",
      "Iteration 128, loss = 0.66641248\n",
      "Iteration 129, loss = 0.66612195\n",
      "Iteration 130, loss = 0.66583127\n",
      "Iteration 131, loss = 0.66554043\n",
      "Iteration 132, loss = 0.66524942\n",
      "Iteration 133, loss = 0.66495823\n",
      "Iteration 134, loss = 0.66466685\n",
      "Iteration 135, loss = 0.66437526\n",
      "Iteration 136, loss = 0.66408346\n",
      "Iteration 137, loss = 0.66379144\n",
      "Iteration 138, loss = 0.66349917\n",
      "Iteration 139, loss = 0.66320667\n",
      "Iteration 140, loss = 0.66291390\n",
      "Iteration 141, loss = 0.66262086\n",
      "Iteration 142, loss = 0.66232755\n",
      "Iteration 143, loss = 0.66203395\n",
      "Iteration 144, loss = 0.66174004\n",
      "Iteration 145, loss = 0.66144583\n",
      "Iteration 146, loss = 0.66115129\n",
      "Iteration 147, loss = 0.66085642\n",
      "Iteration 148, loss = 0.66056120\n",
      "Iteration 149, loss = 0.66026564\n",
      "Iteration 150, loss = 0.65996971\n",
      "Iteration 151, loss = 0.65967340\n",
      "Iteration 152, loss = 0.65937671\n",
      "Iteration 153, loss = 0.65907963\n",
      "Iteration 154, loss = 0.65878214\n",
      "Iteration 155, loss = 0.65848424\n",
      "Iteration 156, loss = 0.65818591\n",
      "Iteration 157, loss = 0.65788715\n",
      "Iteration 158, loss = 0.65758794\n",
      "Iteration 159, loss = 0.65728828\n",
      "Iteration 160, loss = 0.65698816\n",
      "Iteration 161, loss = 0.65668756\n",
      "Iteration 162, loss = 0.65638647\n",
      "Iteration 163, loss = 0.65608490\n",
      "Iteration 164, loss = 0.65578282\n",
      "Iteration 165, loss = 0.65548022\n",
      "Iteration 166, loss = 0.65517711\n",
      "Iteration 167, loss = 0.65487347\n",
      "Iteration 168, loss = 0.65456928\n",
      "Iteration 169, loss = 0.65426455\n",
      "Iteration 170, loss = 0.65395925\n",
      "Iteration 171, loss = 0.65365339\n",
      "Iteration 172, loss = 0.65334695\n",
      "Iteration 173, loss = 0.65303993\n",
      "Iteration 174, loss = 0.65273231\n",
      "Iteration 175, loss = 0.65242409\n",
      "Iteration 176, loss = 0.65211526\n",
      "Iteration 177, loss = 0.65180580\n",
      "Iteration 178, loss = 0.65149572\n",
      "Iteration 179, loss = 0.65118500\n",
      "Iteration 180, loss = 0.65087363\n",
      "Iteration 181, loss = 0.65056160\n",
      "Iteration 182, loss = 0.65024892\n",
      "Iteration 183, loss = 0.64993556\n",
      "Iteration 184, loss = 0.64962153\n",
      "Iteration 185, loss = 0.64930681\n",
      "Iteration 186, loss = 0.64899139\n",
      "Iteration 187, loss = 0.64867527\n",
      "Iteration 188, loss = 0.64835844\n",
      "Iteration 189, loss = 0.64804089\n",
      "Iteration 190, loss = 0.64772261\n",
      "Iteration 191, loss = 0.64740360\n",
      "Iteration 192, loss = 0.64708385\n",
      "Iteration 193, loss = 0.64676335\n",
      "Iteration 194, loss = 0.64644210\n",
      "Iteration 195, loss = 0.64612008\n",
      "Iteration 196, loss = 0.64579729\n",
      "Iteration 197, loss = 0.64547372\n",
      "Iteration 198, loss = 0.64514937\n",
      "Iteration 199, loss = 0.64482423\n",
      "Iteration 200, loss = 0.64449829\n",
      "Iteration 201, loss = 0.64417155\n",
      "Iteration 202, loss = 0.64384399\n",
      "Iteration 203, loss = 0.64351562\n",
      "Iteration 204, loss = 0.64318642\n",
      "Iteration 205, loss = 0.64285639\n",
      "Iteration 206, loss = 0.64252552\n",
      "Iteration 207, loss = 0.64219381\n",
      "Iteration 208, loss = 0.64186125\n",
      "Iteration 209, loss = 0.64152784\n",
      "Iteration 210, loss = 0.64119356\n",
      "Iteration 211, loss = 0.64085842\n",
      "Iteration 212, loss = 0.64052240\n",
      "Iteration 213, loss = 0.64018550\n",
      "Iteration 214, loss = 0.63984772\n",
      "Iteration 215, loss = 0.63950905\n",
      "Iteration 216, loss = 0.63916948\n",
      "Iteration 217, loss = 0.63882901\n",
      "Iteration 218, loss = 0.63848763\n",
      "Iteration 219, loss = 0.63814534\n",
      "Iteration 220, loss = 0.63780213\n",
      "Iteration 221, loss = 0.63745800\n",
      "Iteration 222, loss = 0.63711295\n",
      "Iteration 223, loss = 0.63676696\n",
      "Iteration 224, loss = 0.63642003\n",
      "Iteration 225, loss = 0.63607217\n",
      "Iteration 226, loss = 0.63572335\n",
      "Iteration 227, loss = 0.63537359\n",
      "Iteration 228, loss = 0.63502287\n",
      "Iteration 229, loss = 0.63467119\n",
      "Iteration 230, loss = 0.63431854\n",
      "Iteration 231, loss = 0.63396493\n",
      "Iteration 232, loss = 0.63361034\n",
      "Iteration 233, loss = 0.63325477\n",
      "Iteration 234, loss = 0.63289823\n",
      "Iteration 235, loss = 0.63254070\n",
      "Iteration 236, loss = 0.63218218\n",
      "Iteration 237, loss = 0.63182266\n",
      "Iteration 238, loss = 0.63146215\n",
      "Iteration 239, loss = 0.63110064\n",
      "Iteration 240, loss = 0.63073813\n",
      "Iteration 241, loss = 0.63037461\n",
      "Iteration 242, loss = 0.63001007\n",
      "Iteration 243, loss = 0.62964452\n",
      "Iteration 244, loss = 0.62927796\n",
      "Iteration 245, loss = 0.62891037\n",
      "Iteration 246, loss = 0.62854176\n",
      "Iteration 247, loss = 0.62817213\n",
      "Iteration 248, loss = 0.62780146\n",
      "Iteration 249, loss = 0.62742976\n",
      "Iteration 250, loss = 0.62705702\n",
      "Iteration 251, loss = 0.62668325\n",
      "Iteration 252, loss = 0.62630843\n",
      "Iteration 253, loss = 0.62593257\n",
      "Iteration 254, loss = 0.62555566\n",
      "Iteration 255, loss = 0.62517770\n",
      "Iteration 256, loss = 0.62479869\n",
      "Iteration 257, loss = 0.62441863\n",
      "Iteration 258, loss = 0.62403751\n",
      "Iteration 259, loss = 0.62365533\n",
      "Iteration 260, loss = 0.62327209\n",
      "Iteration 261, loss = 0.62288778\n",
      "Iteration 262, loss = 0.62250242\n",
      "Iteration 263, loss = 0.62211598\n",
      "Iteration 264, loss = 0.62172847\n",
      "Iteration 265, loss = 0.62133990\n",
      "Iteration 266, loss = 0.62095025\n",
      "Iteration 267, loss = 0.62055952\n",
      "Iteration 268, loss = 0.62016772\n",
      "Iteration 269, loss = 0.61977485\n",
      "Iteration 270, loss = 0.61938089\n",
      "Iteration 271, loss = 0.61898585\n",
      "Iteration 272, loss = 0.61858973\n",
      "Iteration 273, loss = 0.61819252\n",
      "Iteration 274, loss = 0.61779423\n",
      "Iteration 275, loss = 0.61739485\n",
      "Iteration 276, loss = 0.61699439\n",
      "Iteration 277, loss = 0.61659283\n",
      "Iteration 278, loss = 0.61619019\n",
      "Iteration 279, loss = 0.61578645\n",
      "Iteration 280, loss = 0.61538162\n",
      "Iteration 281, loss = 0.61497570\n",
      "Iteration 282, loss = 0.61456869\n",
      "Iteration 283, loss = 0.61416058\n",
      "Iteration 284, loss = 0.61375137\n",
      "Iteration 285, loss = 0.61334107\n",
      "Iteration 286, loss = 0.61292967\n",
      "Iteration 287, loss = 0.61251717\n",
      "Iteration 288, loss = 0.61210358\n",
      "Iteration 289, loss = 0.61168889\n",
      "Iteration 290, loss = 0.61127309\n",
      "Iteration 291, loss = 0.61085620\n",
      "Iteration 292, loss = 0.61043821\n",
      "Iteration 293, loss = 0.61001911\n",
      "Iteration 294, loss = 0.60959892\n",
      "Iteration 295, loss = 0.60917763\n",
      "Iteration 296, loss = 0.60875523\n",
      "Iteration 297, loss = 0.60833173\n",
      "Iteration 298, loss = 0.60790714\n",
      "Iteration 299, loss = 0.60748144\n",
      "Iteration 300, loss = 0.60705464\n",
      "Iteration 301, loss = 0.60662674\n",
      "Iteration 302, loss = 0.60619773\n",
      "Iteration 303, loss = 0.60576763\n",
      "Iteration 304, loss = 0.60533643\n",
      "Iteration 305, loss = 0.60490412\n",
      "Iteration 306, loss = 0.60447072\n",
      "Iteration 307, loss = 0.60403622\n",
      "Iteration 308, loss = 0.60360061\n",
      "Iteration 309, loss = 0.60316391\n",
      "Iteration 310, loss = 0.60272611\n",
      "Iteration 311, loss = 0.60228721\n",
      "Iteration 312, loss = 0.60184722\n",
      "Iteration 313, loss = 0.60140612\n",
      "Iteration 314, loss = 0.60096394\n",
      "Iteration 315, loss = 0.60052065\n",
      "Iteration 316, loss = 0.60007627\n",
      "Iteration 317, loss = 0.59963080\n",
      "Iteration 318, loss = 0.59918423\n",
      "Iteration 319, loss = 0.59873658\n",
      "Iteration 320, loss = 0.59828783\n",
      "Iteration 321, loss = 0.59783799\n",
      "Iteration 322, loss = 0.59738706\n",
      "Iteration 323, loss = 0.59693504\n",
      "Iteration 324, loss = 0.59648194\n",
      "Iteration 325, loss = 0.59602775\n",
      "Iteration 326, loss = 0.59557248\n",
      "Iteration 327, loss = 0.59511612\n",
      "Iteration 328, loss = 0.59465868\n",
      "Iteration 329, loss = 0.59420015\n",
      "Iteration 330, loss = 0.59374055\n",
      "Iteration 331, loss = 0.59327987\n",
      "Iteration 332, loss = 0.59281811\n",
      "Iteration 333, loss = 0.59235528\n",
      "Iteration 334, loss = 0.59189137\n",
      "Iteration 335, loss = 0.59142639\n",
      "Iteration 336, loss = 0.59096034\n",
      "Iteration 337, loss = 0.59049322\n",
      "Iteration 338, loss = 0.59002503\n",
      "Iteration 339, loss = 0.58955578\n",
      "Iteration 340, loss = 0.58908546\n",
      "Iteration 341, loss = 0.58861407\n",
      "Iteration 342, loss = 0.58814163\n",
      "Iteration 343, loss = 0.58766813\n",
      "Iteration 344, loss = 0.58719357\n",
      "Iteration 345, loss = 0.58671795\n",
      "Iteration 346, loss = 0.58624129\n",
      "Iteration 347, loss = 0.58576357\n",
      "Iteration 348, loss = 0.58528480\n",
      "Iteration 349, loss = 0.58480498\n",
      "Iteration 350, loss = 0.58432412\n",
      "Iteration 351, loss = 0.58384221\n",
      "Iteration 352, loss = 0.58335927\n",
      "Iteration 353, loss = 0.58287528\n",
      "Iteration 354, loss = 0.58239026\n",
      "Iteration 355, loss = 0.58190421\n",
      "Iteration 356, loss = 0.58141712\n",
      "Iteration 357, loss = 0.58092900\n",
      "Iteration 358, loss = 0.58043986\n",
      "Iteration 359, loss = 0.57994969\n",
      "Iteration 360, loss = 0.57945850\n",
      "Iteration 361, loss = 0.57896628\n",
      "Iteration 362, loss = 0.57847306\n",
      "Iteration 363, loss = 0.57797881\n",
      "Iteration 364, loss = 0.57748356\n",
      "Iteration 365, loss = 0.57698729\n",
      "Iteration 366, loss = 0.57649002\n",
      "Iteration 367, loss = 0.57599174\n",
      "Iteration 368, loss = 0.57549247\n",
      "Iteration 369, loss = 0.57499219\n",
      "Iteration 370, loss = 0.57449092\n",
      "Iteration 371, loss = 0.57398866\n",
      "Iteration 372, loss = 0.57348540\n",
      "Iteration 373, loss = 0.57298116\n",
      "Iteration 374, loss = 0.57247594\n",
      "Iteration 375, loss = 0.57196973\n",
      "Iteration 376, loss = 0.57146254\n",
      "Iteration 377, loss = 0.57095438\n",
      "Iteration 378, loss = 0.57044525\n",
      "Iteration 379, loss = 0.56993515\n",
      "Iteration 380, loss = 0.56942409\n",
      "Iteration 381, loss = 0.56891206\n",
      "Iteration 382, loss = 0.56839907\n",
      "Iteration 383, loss = 0.56788513\n",
      "Iteration 384, loss = 0.56737023\n",
      "Iteration 385, loss = 0.56685439\n",
      "Iteration 386, loss = 0.56633760\n",
      "Iteration 387, loss = 0.56581986\n",
      "Iteration 388, loss = 0.56530119\n",
      "Iteration 389, loss = 0.56478158\n",
      "Iteration 390, loss = 0.56426104\n",
      "Iteration 391, loss = 0.56373957\n",
      "Iteration 392, loss = 0.56321718\n",
      "Iteration 393, loss = 0.56269387\n",
      "Iteration 394, loss = 0.56216963\n",
      "Iteration 395, loss = 0.56164449\n",
      "Iteration 396, loss = 0.56111843\n",
      "Iteration 397, loss = 0.56059147\n",
      "Iteration 398, loss = 0.56006360\n",
      "Iteration 399, loss = 0.55953484\n",
      "Iteration 400, loss = 0.55900518\n",
      "Iteration 401, loss = 0.55847463\n",
      "Iteration 402, loss = 0.55794319\n",
      "Iteration 403, loss = 0.55741087\n",
      "Iteration 404, loss = 0.55687767\n",
      "Iteration 405, loss = 0.55634359\n",
      "Iteration 406, loss = 0.55580865\n",
      "Iteration 407, loss = 0.55527283\n",
      "Iteration 408, loss = 0.55473615\n",
      "Iteration 409, loss = 0.55419862\n",
      "Iteration 410, loss = 0.55366023\n",
      "Iteration 411, loss = 0.55312098\n",
      "Iteration 412, loss = 0.55258090\n",
      "Iteration 413, loss = 0.55203997\n",
      "Iteration 414, loss = 0.55149820\n",
      "Iteration 415, loss = 0.55095559\n",
      "Iteration 416, loss = 0.55041216\n",
      "Iteration 417, loss = 0.54986791\n",
      "Iteration 418, loss = 0.54932283\n",
      "Iteration 419, loss = 0.54877694\n",
      "Iteration 420, loss = 0.54823023\n",
      "Iteration 421, loss = 0.54768272\n",
      "Iteration 422, loss = 0.54713440\n",
      "Iteration 423, loss = 0.54658529\n",
      "Iteration 424, loss = 0.54603539\n",
      "Iteration 425, loss = 0.54548469\n",
      "Iteration 426, loss = 0.54493321\n",
      "Iteration 427, loss = 0.54438095\n",
      "Iteration 428, loss = 0.54382792\n",
      "Iteration 429, loss = 0.54327411\n",
      "Iteration 430, loss = 0.54271954\n",
      "Iteration 431, loss = 0.54216421\n",
      "Iteration 432, loss = 0.54160813\n",
      "Iteration 433, loss = 0.54105129\n",
      "Iteration 434, loss = 0.54049371\n",
      "Iteration 435, loss = 0.53993538\n",
      "Iteration 436, loss = 0.53937632\n",
      "Iteration 437, loss = 0.53881653\n",
      "Iteration 438, loss = 0.53825601\n",
      "Iteration 439, loss = 0.53769477\n",
      "Iteration 440, loss = 0.53713281\n",
      "Iteration 441, loss = 0.53657014\n",
      "Iteration 442, loss = 0.53600677\n",
      "Iteration 443, loss = 0.53544269\n",
      "Iteration 444, loss = 0.53487792\n",
      "Iteration 445, loss = 0.53431245\n",
      "Iteration 446, loss = 0.53374630\n",
      "Iteration 447, loss = 0.53317947\n",
      "Iteration 448, loss = 0.53261196\n",
      "Iteration 449, loss = 0.53204378\n",
      "Iteration 450, loss = 0.53147494\n",
      "Iteration 451, loss = 0.53090544\n",
      "Iteration 452, loss = 0.53033528\n",
      "Iteration 453, loss = 0.52976447\n",
      "Iteration 454, loss = 0.52919302\n",
      "Iteration 455, loss = 0.52862093\n",
      "Iteration 456, loss = 0.52804820\n",
      "Iteration 457, loss = 0.52747485\n",
      "Iteration 458, loss = 0.52690088\n",
      "Iteration 459, loss = 0.52632629\n",
      "Iteration 460, loss = 0.52575108\n",
      "Iteration 461, loss = 0.52517527\n",
      "Iteration 462, loss = 0.52459886\n",
      "Iteration 463, loss = 0.52402186\n",
      "Iteration 464, loss = 0.52344426\n",
      "Iteration 465, loss = 0.52286608\n",
      "Iteration 466, loss = 0.52228732\n",
      "Iteration 467, loss = 0.52170799\n",
      "Iteration 468, loss = 0.52112809\n",
      "Iteration 469, loss = 0.52054763\n",
      "Iteration 470, loss = 0.51996661\n",
      "Iteration 471, loss = 0.51938505\n",
      "Iteration 472, loss = 0.51880293\n",
      "Iteration 473, loss = 0.51822028\n",
      "Iteration 474, loss = 0.51763709\n",
      "Iteration 475, loss = 0.51705338\n",
      "Iteration 476, loss = 0.51646914\n",
      "Iteration 477, loss = 0.51588439\n",
      "Iteration 478, loss = 0.51529913\n",
      "Iteration 479, loss = 0.51471336\n",
      "Iteration 480, loss = 0.51412709\n",
      "Iteration 481, loss = 0.51354033\n",
      "Iteration 482, loss = 0.51295308\n",
      "Iteration 483, loss = 0.51236535\n",
      "Iteration 484, loss = 0.51177715\n",
      "Iteration 485, loss = 0.51118847\n",
      "Iteration 486, loss = 0.51059933\n",
      "Iteration 487, loss = 0.51000973\n",
      "Iteration 488, loss = 0.50941968\n",
      "Iteration 489, loss = 0.50882918\n",
      "Iteration 490, loss = 0.50823824\n",
      "Iteration 491, loss = 0.50764687\n",
      "Iteration 492, loss = 0.50705506\n",
      "Iteration 493, loss = 0.50646284\n",
      "Iteration 494, loss = 0.50587019\n",
      "Iteration 495, loss = 0.50527714\n",
      "Iteration 496, loss = 0.50468368\n",
      "Iteration 497, loss = 0.50408982\n",
      "Iteration 498, loss = 0.50349557\n",
      "Iteration 499, loss = 0.50290093\n",
      "Iteration 500, loss = 0.50230591\n",
      "Iteration 501, loss = 0.50171051\n",
      "Iteration 502, loss = 0.50111474\n",
      "Iteration 503, loss = 0.50051861\n",
      "Iteration 504, loss = 0.49992213\n",
      "Iteration 505, loss = 0.49932529\n",
      "Iteration 506, loss = 0.49872810\n",
      "Iteration 507, loss = 0.49813058\n",
      "Iteration 508, loss = 0.49753272\n",
      "Iteration 509, loss = 0.49693454\n",
      "Iteration 510, loss = 0.49633603\n",
      "Iteration 511, loss = 0.49573721\n",
      "Iteration 512, loss = 0.49513808\n",
      "Iteration 513, loss = 0.49453865\n",
      "Iteration 514, loss = 0.49393892\n",
      "Iteration 515, loss = 0.49333890\n",
      "Iteration 516, loss = 0.49273859\n",
      "Iteration 517, loss = 0.49213800\n",
      "Iteration 518, loss = 0.49153714\n",
      "Iteration 519, loss = 0.49093602\n",
      "Iteration 520, loss = 0.49033463\n",
      "Iteration 521, loss = 0.48973299\n",
      "Iteration 522, loss = 0.48913110\n",
      "Iteration 523, loss = 0.48852896\n",
      "Iteration 524, loss = 0.48792659\n",
      "Iteration 525, loss = 0.48732399\n",
      "Iteration 526, loss = 0.48672116\n",
      "Iteration 527, loss = 0.48611811\n",
      "Iteration 528, loss = 0.48551486\n",
      "Iteration 529, loss = 0.48491139\n",
      "Iteration 530, loss = 0.48430772\n",
      "Iteration 531, loss = 0.48370386\n",
      "Iteration 532, loss = 0.48309981\n",
      "Iteration 533, loss = 0.48249558\n",
      "Iteration 534, loss = 0.48189117\n",
      "Iteration 535, loss = 0.48128659\n",
      "Iteration 536, loss = 0.48068185\n",
      "Iteration 537, loss = 0.48007695\n",
      "Iteration 538, loss = 0.47947189\n",
      "Iteration 539, loss = 0.47886669\n",
      "Iteration 540, loss = 0.47826134\n",
      "Iteration 541, loss = 0.47765587\n",
      "Iteration 542, loss = 0.47705026\n",
      "Iteration 543, loss = 0.47644453\n",
      "Iteration 544, loss = 0.47583868\n",
      "Iteration 545, loss = 0.47523273\n",
      "Iteration 546, loss = 0.47462666\n",
      "Iteration 547, loss = 0.47402050\n",
      "Iteration 548, loss = 0.47341425\n",
      "Iteration 549, loss = 0.47280790\n",
      "Iteration 550, loss = 0.47220148\n",
      "Iteration 551, loss = 0.47159498\n",
      "Iteration 552, loss = 0.47098841\n",
      "Iteration 553, loss = 0.47038178\n",
      "Iteration 554, loss = 0.46977509\n",
      "Iteration 555, loss = 0.46916834\n",
      "Iteration 556, loss = 0.46856155\n",
      "Iteration 557, loss = 0.46795472\n",
      "Iteration 558, loss = 0.46734786\n",
      "Iteration 559, loss = 0.46674097\n",
      "Iteration 560, loss = 0.46613405\n",
      "Iteration 561, loss = 0.46552711\n",
      "Iteration 562, loss = 0.46492017\n",
      "Iteration 563, loss = 0.46431322\n",
      "Iteration 564, loss = 0.46370626\n",
      "Iteration 565, loss = 0.46309932\n",
      "Iteration 566, loss = 0.46249238\n",
      "Iteration 567, loss = 0.46188546\n",
      "Iteration 568, loss = 0.46127857\n",
      "Iteration 569, loss = 0.46067170\n",
      "Iteration 570, loss = 0.46006487\n",
      "Iteration 571, loss = 0.45945807\n",
      "Iteration 572, loss = 0.45885133\n",
      "Iteration 573, loss = 0.45824463\n",
      "Iteration 574, loss = 0.45763799\n",
      "Iteration 575, loss = 0.45703141\n",
      "Iteration 576, loss = 0.45642489\n",
      "Iteration 577, loss = 0.45581846\n",
      "Iteration 578, loss = 0.45521210\n",
      "Iteration 579, loss = 0.45460582\n",
      "Iteration 580, loss = 0.45399963\n",
      "Iteration 581, loss = 0.45339354\n",
      "Iteration 582, loss = 0.45278755\n",
      "Iteration 583, loss = 0.45218166\n",
      "Iteration 584, loss = 0.45157589\n",
      "Iteration 585, loss = 0.45097023\n",
      "Iteration 586, loss = 0.45036469\n",
      "Iteration 587, loss = 0.44975928\n",
      "Iteration 588, loss = 0.44915401\n",
      "Iteration 589, loss = 0.44854887\n",
      "Iteration 590, loss = 0.44794388\n",
      "Iteration 591, loss = 0.44733903\n",
      "Iteration 592, loss = 0.44673434\n",
      "Iteration 593, loss = 0.44612981\n",
      "Iteration 594, loss = 0.44552544\n",
      "Iteration 595, loss = 0.44492124\n",
      "Iteration 596, loss = 0.44431721\n",
      "Iteration 597, loss = 0.44371337\n",
      "Iteration 598, loss = 0.44310971\n",
      "Iteration 599, loss = 0.44250624\n",
      "Iteration 600, loss = 0.44190297\n",
      "Iteration 601, loss = 0.44129989\n",
      "Iteration 602, loss = 0.44069702\n",
      "Iteration 603, loss = 0.44009437\n",
      "Iteration 604, loss = 0.43949192\n",
      "Iteration 605, loss = 0.43888970\n",
      "Iteration 606, loss = 0.43828770\n",
      "Iteration 607, loss = 0.43768594\n",
      "Iteration 608, loss = 0.43708441\n",
      "Iteration 609, loss = 0.43648311\n",
      "Iteration 610, loss = 0.43588207\n",
      "Iteration 611, loss = 0.43528127\n",
      "Iteration 612, loss = 0.43468073\n",
      "Iteration 613, loss = 0.43408045\n",
      "Iteration 614, loss = 0.43348043\n",
      "Iteration 615, loss = 0.43288068\n",
      "Iteration 616, loss = 0.43228121\n",
      "Iteration 617, loss = 0.43168201\n",
      "Iteration 618, loss = 0.43108310\n",
      "Iteration 619, loss = 0.43048447\n",
      "Iteration 620, loss = 0.42988614\n",
      "Iteration 621, loss = 0.42928810\n",
      "Iteration 622, loss = 0.42869037\n",
      "Iteration 623, loss = 0.42809294\n",
      "Iteration 624, loss = 0.42749582\n",
      "Iteration 625, loss = 0.42689902\n",
      "Iteration 626, loss = 0.42630254\n",
      "Iteration 627, loss = 0.42570638\n",
      "Iteration 628, loss = 0.42511055\n",
      "Iteration 629, loss = 0.42451505\n",
      "Iteration 630, loss = 0.42391990\n",
      "Iteration 631, loss = 0.42332508\n",
      "Iteration 632, loss = 0.42273061\n",
      "Iteration 633, loss = 0.42213649\n",
      "Iteration 634, loss = 0.42154273\n",
      "Iteration 635, loss = 0.42094932\n",
      "Iteration 636, loss = 0.42035628\n",
      "Iteration 637, loss = 0.41976361\n",
      "Iteration 638, loss = 0.41917131\n",
      "Iteration 639, loss = 0.41857938\n",
      "Iteration 640, loss = 0.41798784\n",
      "Iteration 641, loss = 0.41739668\n",
      "Iteration 642, loss = 0.41680590\n",
      "Iteration 643, loss = 0.41621552\n",
      "Iteration 644, loss = 0.41562554\n",
      "Iteration 645, loss = 0.41503596\n",
      "Iteration 646, loss = 0.41444678\n",
      "Iteration 647, loss = 0.41385801\n",
      "Iteration 648, loss = 0.41326965\n",
      "Iteration 649, loss = 0.41268171\n",
      "Iteration 650, loss = 0.41209419\n",
      "Iteration 651, loss = 0.41150710\n",
      "Iteration 652, loss = 0.41092043\n",
      "Iteration 653, loss = 0.41033419\n",
      "Iteration 654, loss = 0.40974839\n",
      "Iteration 655, loss = 0.40916303\n",
      "Iteration 656, loss = 0.40857812\n",
      "Iteration 657, loss = 0.40799365\n",
      "Iteration 658, loss = 0.40740963\n",
      "Iteration 659, loss = 0.40682606\n",
      "Iteration 660, loss = 0.40624295\n",
      "Iteration 661, loss = 0.40566031\n",
      "Iteration 662, loss = 0.40507812\n",
      "Iteration 663, loss = 0.40449641\n",
      "Iteration 664, loss = 0.40391517\n",
      "Iteration 665, loss = 0.40333441\n",
      "Iteration 666, loss = 0.40275412\n",
      "Iteration 667, loss = 0.40217432\n",
      "Iteration 668, loss = 0.40159500\n",
      "Iteration 669, loss = 0.40101617\n",
      "Iteration 670, loss = 0.40043784\n",
      "Iteration 671, loss = 0.39986000\n",
      "Iteration 672, loss = 0.39928266\n",
      "Iteration 673, loss = 0.39870582\n",
      "Iteration 674, loss = 0.39812949\n",
      "Iteration 675, loss = 0.39755367\n",
      "Iteration 676, loss = 0.39697836\n",
      "Iteration 677, loss = 0.39640357\n",
      "Iteration 678, loss = 0.39582929\n",
      "Iteration 679, loss = 0.39525554\n",
      "Iteration 680, loss = 0.39468231\n",
      "Iteration 681, loss = 0.39410961\n",
      "Iteration 682, loss = 0.39353745\n",
      "Iteration 683, loss = 0.39296581\n",
      "Iteration 684, loss = 0.39239472\n",
      "Iteration 685, loss = 0.39182416\n",
      "Iteration 686, loss = 0.39125415\n",
      "Iteration 687, loss = 0.39068468\n",
      "Iteration 688, loss = 0.39011576\n",
      "Iteration 689, loss = 0.38954740\n",
      "Iteration 690, loss = 0.38897959\n",
      "Iteration 691, loss = 0.38841233\n",
      "Iteration 692, loss = 0.38784564\n",
      "Iteration 693, loss = 0.38727951\n",
      "Iteration 694, loss = 0.38671395\n",
      "Iteration 695, loss = 0.38614895\n",
      "Iteration 696, loss = 0.38558453\n",
      "Iteration 697, loss = 0.38502068\n",
      "Iteration 698, loss = 0.38445741\n",
      "Iteration 699, loss = 0.38389472\n",
      "Iteration 700, loss = 0.38333261\n",
      "Iteration 701, loss = 0.38277108\n",
      "Iteration 702, loss = 0.38221014\n",
      "Iteration 703, loss = 0.38164979\n",
      "Iteration 704, loss = 0.38109003\n",
      "Iteration 705, loss = 0.38053087\n",
      "Iteration 706, loss = 0.37997231\n",
      "Iteration 707, loss = 0.37941434\n",
      "Iteration 708, loss = 0.37885698\n",
      "Iteration 709, loss = 0.37830022\n",
      "Iteration 710, loss = 0.37774406\n",
      "Iteration 711, loss = 0.37718852\n",
      "Iteration 712, loss = 0.37663359\n",
      "Iteration 713, loss = 0.37607927\n",
      "Iteration 714, loss = 0.37552557\n",
      "Iteration 715, loss = 0.37497248\n",
      "Iteration 716, loss = 0.37442002\n",
      "Iteration 717, loss = 0.37386817\n",
      "Iteration 718, loss = 0.37331696\n",
      "Iteration 719, loss = 0.37276637\n",
      "Iteration 720, loss = 0.37221641\n",
      "Iteration 721, loss = 0.37166708\n",
      "Iteration 722, loss = 0.37111838\n",
      "Iteration 723, loss = 0.37057032\n",
      "Iteration 724, loss = 0.37002289\n",
      "Iteration 725, loss = 0.36947611\n",
      "Iteration 726, loss = 0.36892997\n",
      "Iteration 727, loss = 0.36838446\n",
      "Iteration 728, loss = 0.36783961\n",
      "Iteration 729, loss = 0.36729540\n",
      "Iteration 730, loss = 0.36675184\n",
      "Iteration 731, loss = 0.36620893\n",
      "Iteration 732, loss = 0.36566668\n",
      "Iteration 733, loss = 0.36512508\n",
      "Iteration 734, loss = 0.36458413\n",
      "Iteration 735, loss = 0.36404384\n",
      "Iteration 736, loss = 0.36350422\n",
      "Iteration 737, loss = 0.36296525\n",
      "Iteration 738, loss = 0.36242695\n",
      "Iteration 739, loss = 0.36188931\n",
      "Iteration 740, loss = 0.36135234\n",
      "Iteration 741, loss = 0.36081604\n",
      "Iteration 742, loss = 0.36028040\n",
      "Iteration 743, loss = 0.35974544\n",
      "Iteration 744, loss = 0.35921116\n",
      "Iteration 745, loss = 0.35867754\n",
      "Iteration 746, loss = 0.35814461\n",
      "Iteration 747, loss = 0.35761235\n",
      "Iteration 748, loss = 0.35708077\n",
      "Iteration 749, loss = 0.35654987\n",
      "Iteration 750, loss = 0.35601965\n",
      "Iteration 751, loss = 0.35549011\n",
      "Iteration 752, loss = 0.35496127\n",
      "Iteration 753, loss = 0.35443310\n",
      "Iteration 754, loss = 0.35390563\n",
      "Iteration 755, loss = 0.35337884\n",
      "Iteration 756, loss = 0.35285275\n",
      "Iteration 757, loss = 0.35232734\n",
      "Iteration 758, loss = 0.35180263\n",
      "Iteration 759, loss = 0.35127862\n",
      "Iteration 760, loss = 0.35075530\n",
      "Iteration 761, loss = 0.35023267\n",
      "Iteration 762, loss = 0.34971075\n",
      "Iteration 763, loss = 0.34918952\n",
      "Iteration 764, loss = 0.34866900\n",
      "Iteration 765, loss = 0.34814917\n",
      "Iteration 766, loss = 0.34763005\n",
      "Iteration 767, loss = 0.34711163\n",
      "Iteration 768, loss = 0.34659392\n",
      "Iteration 769, loss = 0.34607691\n",
      "Iteration 770, loss = 0.34556061\n",
      "Iteration 771, loss = 0.34504502\n",
      "Iteration 772, loss = 0.34453014\n",
      "Iteration 773, loss = 0.34401597\n",
      "Iteration 774, loss = 0.34350251\n",
      "Iteration 775, loss = 0.34298976\n",
      "Iteration 776, loss = 0.34247772\n",
      "Iteration 777, loss = 0.34196640\n",
      "Iteration 778, loss = 0.34145579\n",
      "Iteration 779, loss = 0.34094590\n",
      "Iteration 780, loss = 0.34043673\n",
      "Iteration 781, loss = 0.33992827\n",
      "Iteration 782, loss = 0.33942053\n",
      "Iteration 783, loss = 0.33891351\n",
      "Iteration 784, loss = 0.33840721\n",
      "Iteration 785, loss = 0.33790163\n",
      "Iteration 786, loss = 0.33739677\n",
      "Iteration 787, loss = 0.33689264\n",
      "Iteration 788, loss = 0.33638923\n",
      "Iteration 789, loss = 0.33588654\n",
      "Iteration 790, loss = 0.33538458\n",
      "Iteration 791, loss = 0.33488334\n",
      "Iteration 792, loss = 0.33438283\n",
      "Iteration 793, loss = 0.33388304\n",
      "Iteration 794, loss = 0.33338398\n",
      "Iteration 795, loss = 0.33288565\n",
      "Iteration 796, loss = 0.33238805\n",
      "Iteration 797, loss = 0.33189117\n",
      "Iteration 798, loss = 0.33139503\n",
      "Iteration 799, loss = 0.33089962\n",
      "Iteration 800, loss = 0.33040493\n",
      "Iteration 801, loss = 0.32991098\n",
      "Iteration 802, loss = 0.32941776\n",
      "Iteration 803, loss = 0.32892527\n",
      "Iteration 804, loss = 0.32843352\n",
      "Iteration 805, loss = 0.32794250\n",
      "Iteration 806, loss = 0.32745221\n",
      "Iteration 807, loss = 0.32696265\n",
      "Iteration 808, loss = 0.32647383\n",
      "Iteration 809, loss = 0.32598575\n",
      "Iteration 810, loss = 0.32549840\n",
      "Iteration 811, loss = 0.32501179\n",
      "Iteration 812, loss = 0.32452591\n",
      "Iteration 813, loss = 0.32404077\n",
      "Iteration 814, loss = 0.32355636\n",
      "Iteration 815, loss = 0.32307270\n",
      "Iteration 816, loss = 0.32258977\n",
      "Iteration 817, loss = 0.32210757\n",
      "Iteration 818, loss = 0.32162612\n",
      "Iteration 819, loss = 0.32114540\n",
      "Iteration 820, loss = 0.32066542\n",
      "Iteration 821, loss = 0.32018619\n",
      "Iteration 822, loss = 0.31970769\n",
      "Iteration 823, loss = 0.31922992\n",
      "Iteration 824, loss = 0.31875290\n",
      "Iteration 825, loss = 0.31827662\n",
      "Iteration 826, loss = 0.31780108\n",
      "Iteration 827, loss = 0.31732628\n",
      "Iteration 828, loss = 0.31685221\n",
      "Iteration 829, loss = 0.31637889\n",
      "Iteration 830, loss = 0.31590631\n",
      "Iteration 831, loss = 0.31543447\n",
      "Iteration 832, loss = 0.31496337\n",
      "Iteration 833, loss = 0.31449301\n",
      "Iteration 834, loss = 0.31402339\n",
      "Iteration 835, loss = 0.31355451\n",
      "Iteration 836, loss = 0.31308637\n",
      "Iteration 837, loss = 0.31261897\n",
      "Iteration 838, loss = 0.31215232\n",
      "Iteration 839, loss = 0.31168640\n",
      "Iteration 840, loss = 0.31122123\n",
      "Iteration 841, loss = 0.31075679\n",
      "Iteration 842, loss = 0.31029310\n",
      "Iteration 843, loss = 0.30983015\n",
      "Iteration 844, loss = 0.30936794\n",
      "Iteration 845, loss = 0.30890646\n",
      "Iteration 846, loss = 0.30844573\n",
      "Iteration 847, loss = 0.30798574\n",
      "Iteration 848, loss = 0.30752649\n",
      "Iteration 849, loss = 0.30706799\n",
      "Iteration 850, loss = 0.30661022\n",
      "Iteration 851, loss = 0.30615319\n",
      "Iteration 852, loss = 0.30569690\n",
      "Iteration 853, loss = 0.30524135\n",
      "Iteration 854, loss = 0.30478654\n",
      "Iteration 855, loss = 0.30433247\n",
      "Iteration 856, loss = 0.30387914\n",
      "Iteration 857, loss = 0.30342655\n",
      "Iteration 858, loss = 0.30297470\n",
      "Iteration 859, loss = 0.30252358\n",
      "Iteration 860, loss = 0.30207321\n",
      "Iteration 861, loss = 0.30162357\n",
      "Iteration 862, loss = 0.30117467\n",
      "Iteration 863, loss = 0.30072651\n",
      "Iteration 864, loss = 0.30027909\n",
      "Iteration 865, loss = 0.29983241\n",
      "Iteration 866, loss = 0.29938646\n",
      "Iteration 867, loss = 0.29894124\n",
      "Iteration 868, loss = 0.29849677\n",
      "Iteration 869, loss = 0.29805303\n",
      "Iteration 870, loss = 0.29761003\n",
      "Iteration 871, loss = 0.29716776\n",
      "Iteration 872, loss = 0.29672623\n",
      "Iteration 873, loss = 0.29628543\n",
      "Iteration 874, loss = 0.29584537\n",
      "Iteration 875, loss = 0.29540604\n",
      "Iteration 876, loss = 0.29496744\n",
      "Iteration 877, loss = 0.29452958\n",
      "Iteration 878, loss = 0.29409245\n",
      "Iteration 879, loss = 0.29365605\n",
      "Iteration 880, loss = 0.29322039\n",
      "Iteration 881, loss = 0.29278546\n",
      "Iteration 882, loss = 0.29235126\n",
      "Iteration 883, loss = 0.29191779\n",
      "Iteration 884, loss = 0.29148505\n",
      "Iteration 885, loss = 0.29105304\n",
      "Iteration 886, loss = 0.29062176\n",
      "Iteration 887, loss = 0.29019121\n",
      "Iteration 888, loss = 0.28976139\n",
      "Iteration 889, loss = 0.28933230\n",
      "Iteration 890, loss = 0.28890394\n",
      "Iteration 891, loss = 0.28847630\n",
      "Iteration 892, loss = 0.28804939\n",
      "Iteration 893, loss = 0.28762320\n",
      "Iteration 894, loss = 0.28719775\n",
      "Iteration 895, loss = 0.28677301\n",
      "Iteration 896, loss = 0.28634901\n",
      "Iteration 897, loss = 0.28592572\n",
      "Iteration 898, loss = 0.28550316\n",
      "Iteration 899, loss = 0.28508133\n",
      "Iteration 900, loss = 0.28466021\n",
      "Iteration 901, loss = 0.28423982\n",
      "Iteration 902, loss = 0.28382015\n",
      "Iteration 903, loss = 0.28340120\n",
      "Iteration 904, loss = 0.28298297\n",
      "Iteration 905, loss = 0.28256546\n",
      "Iteration 906, loss = 0.28214868\n",
      "Iteration 907, loss = 0.28173260\n",
      "Iteration 908, loss = 0.28131725\n",
      "Iteration 909, loss = 0.28090262\n",
      "Iteration 910, loss = 0.28048870\n",
      "Iteration 911, loss = 0.28007550\n",
      "Iteration 912, loss = 0.27966301\n",
      "Iteration 913, loss = 0.27925124\n",
      "Iteration 914, loss = 0.27884018\n",
      "Iteration 915, loss = 0.27842984\n",
      "Iteration 916, loss = 0.27802021\n",
      "Iteration 917, loss = 0.27761129\n",
      "Iteration 918, loss = 0.27720308\n",
      "Iteration 919, loss = 0.27679559\n",
      "Iteration 920, loss = 0.27638880\n",
      "Iteration 921, loss = 0.27598273\n",
      "Iteration 922, loss = 0.27557736\n",
      "Iteration 923, loss = 0.27517270\n",
      "Iteration 924, loss = 0.27476875\n",
      "Iteration 925, loss = 0.27436551\n",
      "Iteration 926, loss = 0.27396297\n",
      "Iteration 927, loss = 0.27356114\n",
      "Iteration 928, loss = 0.27316002\n",
      "Iteration 929, loss = 0.27275959\n",
      "Iteration 930, loss = 0.27235987\n",
      "Iteration 931, loss = 0.27196086\n",
      "Iteration 932, loss = 0.27156254\n",
      "Iteration 933, loss = 0.27116493\n",
      "Iteration 934, loss = 0.27076802\n",
      "Iteration 935, loss = 0.27037180\n",
      "Iteration 936, loss = 0.26997629\n",
      "Iteration 937, loss = 0.26958147\n",
      "Iteration 938, loss = 0.26918735\n",
      "Iteration 939, loss = 0.26879393\n",
      "Iteration 940, loss = 0.26840120\n",
      "Iteration 941, loss = 0.26800917\n",
      "Iteration 942, loss = 0.26761784\n",
      "Iteration 943, loss = 0.26722719\n",
      "Iteration 944, loss = 0.26683724\n",
      "Iteration 945, loss = 0.26644798\n",
      "Iteration 946, loss = 0.26605942\n",
      "Iteration 947, loss = 0.26567154\n",
      "Iteration 948, loss = 0.26528435\n",
      "Iteration 949, loss = 0.26489785\n",
      "Iteration 950, loss = 0.26451204\n",
      "Iteration 951, loss = 0.26412692\n",
      "Iteration 952, loss = 0.26374248\n",
      "Iteration 953, loss = 0.26335873\n",
      "Iteration 954, loss = 0.26297566\n",
      "Iteration 955, loss = 0.26259328\n",
      "Iteration 956, loss = 0.26221158\n",
      "Iteration 957, loss = 0.26183056\n",
      "Iteration 958, loss = 0.26145022\n",
      "Iteration 959, loss = 0.26107056\n",
      "Iteration 960, loss = 0.26069158\n",
      "Iteration 961, loss = 0.26031329\n",
      "Iteration 962, loss = 0.25993567\n",
      "Iteration 963, loss = 0.25955872\n",
      "Iteration 964, loss = 0.25918245\n",
      "Iteration 965, loss = 0.25880686\n",
      "Iteration 966, loss = 0.25843194\n",
      "Iteration 967, loss = 0.25805770\n",
      "Iteration 968, loss = 0.25768413\n",
      "Iteration 969, loss = 0.25731123\n",
      "Iteration 970, loss = 0.25693900\n",
      "Iteration 971, loss = 0.25656744\n",
      "Iteration 972, loss = 0.25619655\n",
      "Iteration 973, loss = 0.25582633\n",
      "Iteration 974, loss = 0.25545677\n",
      "Iteration 975, loss = 0.25508789\n",
      "Iteration 976, loss = 0.25471966\n",
      "Iteration 977, loss = 0.25435211\n",
      "Iteration 978, loss = 0.25398521\n",
      "Iteration 979, loss = 0.25361898\n",
      "Iteration 980, loss = 0.25325341\n",
      "Iteration 981, loss = 0.25288850\n",
      "Iteration 982, loss = 0.25252426\n",
      "Iteration 983, loss = 0.25216067\n",
      "Iteration 984, loss = 0.25179774\n",
      "Iteration 985, loss = 0.25143546\n",
      "Iteration 986, loss = 0.25107385\n",
      "Iteration 987, loss = 0.25071289\n",
      "Iteration 988, loss = 0.25035258\n",
      "Iteration 989, loss = 0.24999293\n",
      "Iteration 990, loss = 0.24963393\n",
      "Iteration 991, loss = 0.24927558\n",
      "Iteration 992, loss = 0.24891788\n",
      "Iteration 993, loss = 0.24856083\n",
      "Iteration 994, loss = 0.24820443\n",
      "Iteration 995, loss = 0.24784868\n",
      "Iteration 996, loss = 0.24749358\n",
      "Iteration 997, loss = 0.24713912\n",
      "Iteration 998, loss = 0.24678531\n",
      "Iteration 999, loss = 0.24643214\n",
      "Iteration 1000, loss = 0.24607962\n",
      "Iteration 1001, loss = 0.24572774\n",
      "Iteration 1002, loss = 0.24537649\n",
      "Iteration 1003, loss = 0.24502589\n",
      "Iteration 1004, loss = 0.24467593\n",
      "Iteration 1005, loss = 0.24432661\n",
      "Iteration 1006, loss = 0.24397792\n",
      "Iteration 1007, loss = 0.24362988\n",
      "Iteration 1008, loss = 0.24328246\n",
      "Iteration 1009, loss = 0.24293569\n",
      "Iteration 1010, loss = 0.24258954\n",
      "Iteration 1011, loss = 0.24224403\n",
      "Iteration 1012, loss = 0.24189915\n",
      "Iteration 1013, loss = 0.24155490\n",
      "Iteration 1014, loss = 0.24121128\n",
      "Iteration 1015, loss = 0.24086829\n",
      "Iteration 1016, loss = 0.24052593\n",
      "Iteration 1017, loss = 0.24018419\n",
      "Iteration 1018, loss = 0.23984308\n",
      "Iteration 1019, loss = 0.23950260\n",
      "Iteration 1020, loss = 0.23916273\n",
      "Iteration 1021, loss = 0.23882350\n",
      "Iteration 1022, loss = 0.23848488\n",
      "Iteration 1023, loss = 0.23814688\n",
      "Iteration 1024, loss = 0.23780951\n",
      "Iteration 1025, loss = 0.23747275\n",
      "Iteration 1026, loss = 0.23713661\n",
      "Iteration 1027, loss = 0.23680109\n",
      "Iteration 1028, loss = 0.23646619\n",
      "Iteration 1029, loss = 0.23613190\n",
      "Iteration 1030, loss = 0.23579822\n",
      "Iteration 1031, loss = 0.23546516\n",
      "Iteration 1032, loss = 0.23513271\n",
      "Iteration 1033, loss = 0.23480087\n",
      "Iteration 1034, loss = 0.23446964\n",
      "Iteration 1035, loss = 0.23413902\n",
      "Iteration 1036, loss = 0.23380900\n",
      "Iteration 1037, loss = 0.23347960\n",
      "Iteration 1038, loss = 0.23315080\n",
      "Iteration 1039, loss = 0.23282260\n",
      "Iteration 1040, loss = 0.23249501\n",
      "Iteration 1041, loss = 0.23216803\n",
      "Iteration 1042, loss = 0.23184164\n",
      "Iteration 1043, loss = 0.23151586\n",
      "Iteration 1044, loss = 0.23119067\n",
      "Iteration 1045, loss = 0.23086609\n",
      "Iteration 1046, loss = 0.23054210\n",
      "Iteration 1047, loss = 0.23021871\n",
      "Iteration 1048, loss = 0.22989592\n",
      "Iteration 1049, loss = 0.22957372\n",
      "Iteration 1050, loss = 0.22925212\n",
      "Iteration 1051, loss = 0.22893110\n",
      "Iteration 1052, loss = 0.22861068\n",
      "Iteration 1053, loss = 0.22829086\n",
      "Iteration 1054, loss = 0.22797162\n",
      "Iteration 1055, loss = 0.22765297\n",
      "Iteration 1056, loss = 0.22733491\n",
      "Iteration 1057, loss = 0.22701743\n",
      "Iteration 1058, loss = 0.22670055\n",
      "Iteration 1059, loss = 0.22638424\n",
      "Iteration 1060, loss = 0.22606852\n",
      "Iteration 1061, loss = 0.22575339\n",
      "Iteration 1062, loss = 0.22543883\n",
      "Iteration 1063, loss = 0.22512486\n",
      "Iteration 1064, loss = 0.22481147\n",
      "Iteration 1065, loss = 0.22449865\n",
      "Iteration 1066, loss = 0.22418642\n",
      "Iteration 1067, loss = 0.22387476\n",
      "Iteration 1068, loss = 0.22356368\n",
      "Iteration 1069, loss = 0.22325317\n",
      "Iteration 1070, loss = 0.22294323\n",
      "Iteration 1071, loss = 0.22263387\n",
      "Iteration 1072, loss = 0.22232508\n",
      "Iteration 1073, loss = 0.22201687\n",
      "Iteration 1074, loss = 0.22170922\n",
      "Iteration 1075, loss = 0.22140214\n",
      "Iteration 1076, loss = 0.22109563\n",
      "Iteration 1077, loss = 0.22078968\n",
      "Iteration 1078, loss = 0.22048430\n",
      "Iteration 1079, loss = 0.22017949\n",
      "Iteration 1080, loss = 0.21987524\n",
      "Iteration 1081, loss = 0.21957155\n",
      "Iteration 1082, loss = 0.21926843\n",
      "Iteration 1083, loss = 0.21896586\n",
      "Iteration 1084, loss = 0.21866386\n",
      "Iteration 1085, loss = 0.21836241\n",
      "Iteration 1086, loss = 0.21806152\n",
      "Iteration 1087, loss = 0.21776119\n",
      "Iteration 1088, loss = 0.21746142\n",
      "Iteration 1089, loss = 0.21716220\n",
      "Iteration 1090, loss = 0.21686353\n",
      "Iteration 1091, loss = 0.21656542\n",
      "Iteration 1092, loss = 0.21626785\n",
      "Iteration 1093, loss = 0.21597084\n",
      "Iteration 1094, loss = 0.21567438\n",
      "Iteration 1095, loss = 0.21537847\n",
      "Iteration 1096, loss = 0.21508310\n",
      "Iteration 1097, loss = 0.21478829\n",
      "Iteration 1098, loss = 0.21449401\n",
      "Iteration 1099, loss = 0.21420029\n",
      "Iteration 1100, loss = 0.21390710\n",
      "Iteration 1101, loss = 0.21361446\n",
      "Iteration 1102, loss = 0.21332236\n",
      "Iteration 1103, loss = 0.21303081\n",
      "Iteration 1104, loss = 0.21273979\n",
      "Iteration 1105, loss = 0.21244931\n",
      "Iteration 1106, loss = 0.21215937\n",
      "Iteration 1107, loss = 0.21186996\n",
      "Iteration 1108, loss = 0.21158109\n",
      "Iteration 1109, loss = 0.21129276\n",
      "Iteration 1110, loss = 0.21100496\n",
      "Iteration 1111, loss = 0.21071769\n",
      "Iteration 1112, loss = 0.21043096\n",
      "Iteration 1113, loss = 0.21014475\n",
      "Iteration 1114, loss = 0.20985908\n",
      "Iteration 1115, loss = 0.20957393\n",
      "Iteration 1116, loss = 0.20928931\n",
      "Iteration 1117, loss = 0.20900522\n",
      "Iteration 1118, loss = 0.20872166\n",
      "Iteration 1119, loss = 0.20843862\n",
      "Iteration 1120, loss = 0.20815610\n",
      "Iteration 1121, loss = 0.20787411\n",
      "Iteration 1122, loss = 0.20759264\n",
      "Iteration 1123, loss = 0.20731169\n",
      "Iteration 1124, loss = 0.20703126\n",
      "Iteration 1125, loss = 0.20675135\n",
      "Iteration 1126, loss = 0.20647195\n",
      "Iteration 1127, loss = 0.20619308\n",
      "Iteration 1128, loss = 0.20591472\n",
      "Iteration 1129, loss = 0.20563687\n",
      "Iteration 1130, loss = 0.20535954\n",
      "Iteration 1131, loss = 0.20508272\n",
      "Iteration 1132, loss = 0.20480642\n",
      "Iteration 1133, loss = 0.20453062\n",
      "Iteration 1134, loss = 0.20425534\n",
      "Iteration 1135, loss = 0.20398056\n",
      "Iteration 1136, loss = 0.20370630\n",
      "Iteration 1137, loss = 0.20343254\n",
      "Iteration 1138, loss = 0.20315929\n",
      "Iteration 1139, loss = 0.20288654\n",
      "Iteration 1140, loss = 0.20261430\n",
      "Iteration 1141, loss = 0.20234256\n",
      "Iteration 1142, loss = 0.20207132\n",
      "Iteration 1143, loss = 0.20180058\n",
      "Iteration 1144, loss = 0.20153035\n",
      "Iteration 1145, loss = 0.20126061\n",
      "Iteration 1146, loss = 0.20099137\n",
      "Iteration 1147, loss = 0.20072263\n",
      "Iteration 1148, loss = 0.20045439\n",
      "Iteration 1149, loss = 0.20018665\n",
      "Iteration 1150, loss = 0.19991939\n",
      "Iteration 1151, loss = 0.19965263\n",
      "Iteration 1152, loss = 0.19938637\n",
      "Iteration 1153, loss = 0.19912059\n",
      "Iteration 1154, loss = 0.19885531\n",
      "Iteration 1155, loss = 0.19859052\n",
      "Iteration 1156, loss = 0.19832621\n",
      "Iteration 1157, loss = 0.19806240\n",
      "Iteration 1158, loss = 0.19779907\n",
      "Iteration 1159, loss = 0.19753623\n",
      "Iteration 1160, loss = 0.19727387\n",
      "Iteration 1161, loss = 0.19701200\n",
      "Iteration 1162, loss = 0.19675061\n",
      "Iteration 1163, loss = 0.19648970\n",
      "Iteration 1164, loss = 0.19622927\n",
      "Iteration 1165, loss = 0.19596933\n",
      "Iteration 1166, loss = 0.19570986\n",
      "Iteration 1167, loss = 0.19545087\n",
      "Iteration 1168, loss = 0.19519236\n",
      "Iteration 1169, loss = 0.19493433\n",
      "Iteration 1170, loss = 0.19467677\n",
      "Iteration 1171, loss = 0.19441969\n",
      "Iteration 1172, loss = 0.19416308\n",
      "Iteration 1173, loss = 0.19390695\n",
      "Iteration 1174, loss = 0.19365128\n",
      "Iteration 1175, loss = 0.19339609\n",
      "Iteration 1176, loss = 0.19314137\n",
      "Iteration 1177, loss = 0.19288712\n",
      "Iteration 1178, loss = 0.19263333\n",
      "Iteration 1179, loss = 0.19238001\n",
      "Iteration 1180, loss = 0.19212716\n",
      "Iteration 1181, loss = 0.19187478\n",
      "Iteration 1182, loss = 0.19162286\n",
      "Iteration 1183, loss = 0.19137140\n",
      "Iteration 1184, loss = 0.19112041\n",
      "Iteration 1185, loss = 0.19086987\n",
      "Iteration 1186, loss = 0.19061980\n",
      "Iteration 1187, loss = 0.19037019\n",
      "Iteration 1188, loss = 0.19012104\n",
      "Iteration 1189, loss = 0.18987234\n",
      "Iteration 1190, loss = 0.18962411\n",
      "Iteration 1191, loss = 0.18937633\n",
      "Iteration 1192, loss = 0.18912900\n",
      "Iteration 1193, loss = 0.18888213\n",
      "Iteration 1194, loss = 0.18863571\n",
      "Iteration 1195, loss = 0.18838975\n",
      "Iteration 1196, loss = 0.18814424\n",
      "Iteration 1197, loss = 0.18789918\n",
      "Iteration 1198, loss = 0.18765457\n",
      "Iteration 1199, loss = 0.18741040\n",
      "Iteration 1200, loss = 0.18716669\n",
      "Iteration 1201, loss = 0.18692342\n",
      "Iteration 1202, loss = 0.18668060\n",
      "Iteration 1203, loss = 0.18643823\n",
      "Iteration 1204, loss = 0.18619630\n",
      "Iteration 1205, loss = 0.18595481\n",
      "Iteration 1206, loss = 0.18571377\n",
      "Iteration 1207, loss = 0.18547317\n",
      "Iteration 1208, loss = 0.18523301\n",
      "Iteration 1209, loss = 0.18499329\n",
      "Iteration 1210, loss = 0.18475401\n",
      "Iteration 1211, loss = 0.18451516\n",
      "Iteration 1212, loss = 0.18427676\n",
      "Iteration 1213, loss = 0.18403879\n",
      "Iteration 1214, loss = 0.18380126\n",
      "Iteration 1215, loss = 0.18356416\n",
      "Iteration 1216, loss = 0.18332750\n",
      "Iteration 1217, loss = 0.18309127\n",
      "Iteration 1218, loss = 0.18285547\n",
      "Iteration 1219, loss = 0.18262010\n",
      "Iteration 1220, loss = 0.18238516\n",
      "Iteration 1221, loss = 0.18215066\n",
      "Iteration 1222, loss = 0.18191658\n",
      "Iteration 1223, loss = 0.18168293\n",
      "Iteration 1224, loss = 0.18144971\n",
      "Iteration 1225, loss = 0.18121691\n",
      "Iteration 1226, loss = 0.18098454\n",
      "Iteration 1227, loss = 0.18075259\n",
      "Iteration 1228, loss = 0.18052107\n",
      "Iteration 1229, loss = 0.18028997\n",
      "Iteration 1230, loss = 0.18005929\n",
      "Iteration 1231, loss = 0.17982903\n",
      "Iteration 1232, loss = 0.17959919\n",
      "Iteration 1233, loss = 0.17936977\n",
      "Iteration 1234, loss = 0.17914077\n",
      "Iteration 1235, loss = 0.17891219\n",
      "Iteration 1236, loss = 0.17868402\n",
      "Iteration 1237, loss = 0.17845627\n",
      "Iteration 1238, loss = 0.17822894\n",
      "Iteration 1239, loss = 0.17800202\n",
      "Iteration 1240, loss = 0.17777551\n",
      "Iteration 1241, loss = 0.17754941\n",
      "Iteration 1242, loss = 0.17732373\n",
      "Iteration 1243, loss = 0.17709846\n",
      "Iteration 1244, loss = 0.17687359\n",
      "Iteration 1245, loss = 0.17664914\n",
      "Iteration 1246, loss = 0.17642510\n",
      "Iteration 1247, loss = 0.17620146\n",
      "Iteration 1248, loss = 0.17597823\n",
      "Iteration 1249, loss = 0.17575540\n",
      "Iteration 1250, loss = 0.17553298\n",
      "Iteration 1251, loss = 0.17531096\n",
      "Iteration 1252, loss = 0.17508935\n",
      "Iteration 1253, loss = 0.17486814\n",
      "Iteration 1254, loss = 0.17464733\n",
      "Iteration 1255, loss = 0.17442692\n",
      "Iteration 1256, loss = 0.17420692\n",
      "Iteration 1257, loss = 0.17398731\n",
      "Iteration 1258, loss = 0.17376810\n",
      "Iteration 1259, loss = 0.17354928\n",
      "Iteration 1260, loss = 0.17333087\n",
      "Iteration 1261, loss = 0.17311285\n",
      "Iteration 1262, loss = 0.17289522\n",
      "Iteration 1263, loss = 0.17267799\n",
      "Iteration 1264, loss = 0.17246115\n",
      "Iteration 1265, loss = 0.17224471\n",
      "Iteration 1266, loss = 0.17202866\n",
      "Iteration 1267, loss = 0.17181299\n",
      "Iteration 1268, loss = 0.17159772\n",
      "Iteration 1269, loss = 0.17138284\n",
      "Iteration 1270, loss = 0.17116835\n",
      "Iteration 1271, loss = 0.17095424\n",
      "Iteration 1272, loss = 0.17074052\n",
      "Iteration 1273, loss = 0.17052719\n",
      "Iteration 1274, loss = 0.17031424\n",
      "Iteration 1275, loss = 0.17010168\n",
      "Iteration 1276, loss = 0.16988950\n",
      "Iteration 1277, loss = 0.16967771\n",
      "Iteration 1278, loss = 0.16946629\n",
      "Iteration 1279, loss = 0.16925526\n",
      "Iteration 1280, loss = 0.16904461\n",
      "Iteration 1281, loss = 0.16883434\n",
      "Iteration 1282, loss = 0.16862445\n",
      "Iteration 1283, loss = 0.16841494\n",
      "Iteration 1284, loss = 0.16820580\n",
      "Iteration 1285, loss = 0.16799704\n",
      "Iteration 1286, loss = 0.16778866\n",
      "Iteration 1287, loss = 0.16758065\n",
      "Iteration 1288, loss = 0.16737302\n",
      "Iteration 1289, loss = 0.16716576\n",
      "Iteration 1290, loss = 0.16695887\n",
      "Iteration 1291, loss = 0.16675236\n",
      "Iteration 1292, loss = 0.16654621\n",
      "Iteration 1293, loss = 0.16634044\n",
      "Iteration 1294, loss = 0.16613504\n",
      "Iteration 1295, loss = 0.16593001\n",
      "Iteration 1296, loss = 0.16572534\n",
      "Iteration 1297, loss = 0.16552104\n",
      "Iteration 1298, loss = 0.16531711\n",
      "Iteration 1299, loss = 0.16511355\n",
      "Iteration 1300, loss = 0.16491035\n",
      "Iteration 1301, loss = 0.16470751\n",
      "Iteration 1302, loss = 0.16450504\n",
      "Iteration 1303, loss = 0.16430293\n",
      "Iteration 1304, loss = 0.16410119\n",
      "Iteration 1305, loss = 0.16389980\n",
      "Iteration 1306, loss = 0.16369878\n",
      "Iteration 1307, loss = 0.16349812\n",
      "Iteration 1308, loss = 0.16329781\n",
      "Iteration 1309, loss = 0.16309786\n",
      "Iteration 1310, loss = 0.16289828\n",
      "Iteration 1311, loss = 0.16269905\n",
      "Iteration 1312, loss = 0.16250017\n",
      "Iteration 1313, loss = 0.16230165\n",
      "Iteration 1314, loss = 0.16210349\n",
      "Iteration 1315, loss = 0.16190568\n",
      "Iteration 1316, loss = 0.16170822\n",
      "Iteration 1317, loss = 0.16151111\n",
      "Iteration 1318, loss = 0.16131436\n",
      "Iteration 1319, loss = 0.16111796\n",
      "Iteration 1320, loss = 0.16092191\n",
      "Iteration 1321, loss = 0.16072621\n",
      "Iteration 1322, loss = 0.16053086\n",
      "Iteration 1323, loss = 0.16033585\n",
      "Iteration 1324, loss = 0.16014120\n",
      "Iteration 1325, loss = 0.15994689\n",
      "Iteration 1326, loss = 0.15975292\n",
      "Iteration 1327, loss = 0.15955930\n",
      "Iteration 1328, loss = 0.15936603\n",
      "Iteration 1329, loss = 0.15917310\n",
      "Iteration 1330, loss = 0.15898051\n",
      "Iteration 1331, loss = 0.15878827\n",
      "Iteration 1332, loss = 0.15859637\n",
      "Iteration 1333, loss = 0.15840480\n",
      "Iteration 1334, loss = 0.15821358\n",
      "Iteration 1335, loss = 0.15802270\n",
      "Iteration 1336, loss = 0.15783216\n",
      "Iteration 1337, loss = 0.15764196\n",
      "Iteration 1338, loss = 0.15745209\n",
      "Iteration 1339, loss = 0.15726256\n",
      "Iteration 1340, loss = 0.15707337\n",
      "Iteration 1341, loss = 0.15688451\n",
      "Iteration 1342, loss = 0.15669598\n",
      "Iteration 1343, loss = 0.15650779\n",
      "Iteration 1344, loss = 0.15631994\n",
      "Iteration 1345, loss = 0.15613242\n",
      "Iteration 1346, loss = 0.15594522\n",
      "Iteration 1347, loss = 0.15575836\n",
      "Iteration 1348, loss = 0.15557183\n",
      "Iteration 1349, loss = 0.15538563\n",
      "Iteration 1350, loss = 0.15519976\n",
      "Iteration 1351, loss = 0.15501422\n",
      "Iteration 1352, loss = 0.15482901\n",
      "Iteration 1353, loss = 0.15464412\n",
      "Iteration 1354, loss = 0.15445956\n",
      "Iteration 1355, loss = 0.15427533\n",
      "Iteration 1356, loss = 0.15409142\n",
      "Iteration 1357, loss = 0.15390783\n",
      "Iteration 1358, loss = 0.15372457\n",
      "Iteration 1359, loss = 0.15354163\n",
      "Iteration 1360, loss = 0.15335902\n",
      "Iteration 1361, loss = 0.15317672\n",
      "Iteration 1362, loss = 0.15299475\n",
      "Iteration 1363, loss = 0.15281310\n",
      "Iteration 1364, loss = 0.15263176\n",
      "Iteration 1365, loss = 0.15245075\n",
      "Iteration 1366, loss = 0.15227005\n",
      "Iteration 1367, loss = 0.15208968\n",
      "Iteration 1368, loss = 0.15190961\n",
      "Iteration 1369, loss = 0.15172987\n",
      "Iteration 1370, loss = 0.15155044\n",
      "Iteration 1371, loss = 0.15137133\n",
      "Iteration 1372, loss = 0.15119253\n",
      "Iteration 1373, loss = 0.15101404\n",
      "Iteration 1374, loss = 0.15083587\n",
      "Iteration 1375, loss = 0.15065801\n",
      "Iteration 1376, loss = 0.15048047\n",
      "Iteration 1377, loss = 0.15030323\n",
      "Iteration 1378, loss = 0.15012630\n",
      "Iteration 1379, loss = 0.14994969\n",
      "Iteration 1380, loss = 0.14977338\n",
      "Iteration 1381, loss = 0.14959738\n",
      "Iteration 1382, loss = 0.14942169\n",
      "Iteration 1383, loss = 0.14924631\n",
      "Iteration 1384, loss = 0.14907123\n",
      "Iteration 1385, loss = 0.14889646\n",
      "Iteration 1386, loss = 0.14872200\n",
      "Iteration 1387, loss = 0.14854784\n",
      "Iteration 1388, loss = 0.14837398\n",
      "Iteration 1389, loss = 0.14820043\n",
      "Iteration 1390, loss = 0.14802718\n",
      "Iteration 1391, loss = 0.14785424\n",
      "Iteration 1392, loss = 0.14768159\n",
      "Iteration 1393, loss = 0.14750925\n",
      "Iteration 1394, loss = 0.14733720\n",
      "Iteration 1395, loss = 0.14716546\n",
      "Iteration 1396, loss = 0.14699401\n",
      "Iteration 1397, loss = 0.14682287\n",
      "Iteration 1398, loss = 0.14665202\n",
      "Iteration 1399, loss = 0.14648147\n",
      "Iteration 1400, loss = 0.14631121\n",
      "Iteration 1401, loss = 0.14614125\n",
      "Iteration 1402, loss = 0.14597159\n",
      "Iteration 1403, loss = 0.14580222\n",
      "Iteration 1404, loss = 0.14563315\n",
      "Iteration 1405, loss = 0.14546437\n",
      "Iteration 1406, loss = 0.14529588\n",
      "Iteration 1407, loss = 0.14512768\n",
      "Iteration 1408, loss = 0.14495978\n",
      "Iteration 1409, loss = 0.14479217\n",
      "Iteration 1410, loss = 0.14462484\n",
      "Iteration 1411, loss = 0.14445781\n",
      "Iteration 1412, loss = 0.14429107\n",
      "Iteration 1413, loss = 0.14412462\n",
      "Iteration 1414, loss = 0.14395845\n",
      "Iteration 1415, loss = 0.14379257\n",
      "Iteration 1416, loss = 0.14362698\n",
      "Iteration 1417, loss = 0.14346167\n",
      "Iteration 1418, loss = 0.14329665\n",
      "Iteration 1419, loss = 0.14313192\n",
      "Iteration 1420, loss = 0.14296747\n",
      "Iteration 1421, loss = 0.14280331\n",
      "Iteration 1422, loss = 0.14263942\n",
      "Iteration 1423, loss = 0.14247582\n",
      "Iteration 1424, loss = 0.14231251\n",
      "Iteration 1425, loss = 0.14214947\n",
      "Iteration 1426, loss = 0.14198672\n",
      "Iteration 1427, loss = 0.14182424\n",
      "Iteration 1428, loss = 0.14166205\n",
      "Iteration 1429, loss = 0.14150013\n",
      "Iteration 1430, loss = 0.14133849\n",
      "Iteration 1431, loss = 0.14117714\n",
      "Iteration 1432, loss = 0.14101606\n",
      "Iteration 1433, loss = 0.14085525\n",
      "Iteration 1434, loss = 0.14069472\n",
      "Iteration 1435, loss = 0.14053447\n",
      "Iteration 1436, loss = 0.14037450\n",
      "Iteration 1437, loss = 0.14021479\n",
      "Iteration 1438, loss = 0.14005537\n",
      "Iteration 1439, loss = 0.13989621\n",
      "Iteration 1440, loss = 0.13973733\n",
      "Iteration 1441, loss = 0.13957872\n",
      "Iteration 1442, loss = 0.13942039\n",
      "Iteration 1443, loss = 0.13926232\n",
      "Iteration 1444, loss = 0.13910453\n",
      "Iteration 1445, loss = 0.13894700\n",
      "Iteration 1446, loss = 0.13878975\n",
      "Iteration 1447, loss = 0.13863276\n",
      "Iteration 1448, loss = 0.13847604\n",
      "Iteration 1449, loss = 0.13831959\n",
      "Iteration 1450, loss = 0.13816341\n",
      "Iteration 1451, loss = 0.13800750\n",
      "Iteration 1452, loss = 0.13785185\n",
      "Iteration 1453, loss = 0.13769646\n",
      "Iteration 1454, loss = 0.13754135\n",
      "Iteration 1455, loss = 0.13738649\n",
      "Iteration 1456, loss = 0.13723190\n",
      "Iteration 1457, loss = 0.13707758\n",
      "Iteration 1458, loss = 0.13692351\n",
      "Iteration 1459, loss = 0.13676971\n",
      "Iteration 1460, loss = 0.13661617\n",
      "Iteration 1461, loss = 0.13646290\n",
      "Iteration 1462, loss = 0.13630988\n",
      "Iteration 1463, loss = 0.13615712\n",
      "Iteration 1464, loss = 0.13600463\n",
      "Iteration 1465, loss = 0.13585239\n",
      "Iteration 1466, loss = 0.13570041\n",
      "Iteration 1467, loss = 0.13554869\n",
      "Iteration 1468, loss = 0.13539723\n",
      "Iteration 1469, loss = 0.13524602\n",
      "Iteration 1470, loss = 0.13509507\n",
      "Iteration 1471, loss = 0.13494438\n",
      "Iteration 1472, loss = 0.13479394\n",
      "Iteration 1473, loss = 0.13464376\n",
      "Iteration 1474, loss = 0.13449383\n",
      "Iteration 1475, loss = 0.13434416\n",
      "Iteration 1476, loss = 0.13419474\n",
      "Iteration 1477, loss = 0.13404557\n",
      "Iteration 1478, loss = 0.13389665\n",
      "Iteration 1479, loss = 0.13374799\n",
      "Iteration 1480, loss = 0.13359958\n",
      "Iteration 1481, loss = 0.13345141\n",
      "Iteration 1482, loss = 0.13330350\n",
      "Iteration 1483, loss = 0.13315584\n",
      "Iteration 1484, loss = 0.13300843\n",
      "Iteration 1485, loss = 0.13286126\n",
      "Iteration 1486, loss = 0.13271435\n",
      "Iteration 1487, loss = 0.13256768\n",
      "Iteration 1488, loss = 0.13242126\n",
      "Iteration 1489, loss = 0.13227508\n",
      "Iteration 1490, loss = 0.13212916\n",
      "Iteration 1491, loss = 0.13198347\n",
      "Iteration 1492, loss = 0.13183804\n",
      "Iteration 1493, loss = 0.13169284\n",
      "Iteration 1494, loss = 0.13154789\n",
      "Iteration 1495, loss = 0.13140319\n",
      "Iteration 1496, loss = 0.13125873\n",
      "Iteration 1497, loss = 0.13111451\n",
      "Iteration 1498, loss = 0.13097053\n",
      "Iteration 1499, loss = 0.13082680\n",
      "Iteration 1500, loss = 0.13068330\n",
      "Iteration 1501, loss = 0.13054005\n",
      "Iteration 1502, loss = 0.13039704\n",
      "Iteration 1503, loss = 0.13025426\n",
      "Iteration 1504, loss = 0.13011173\n",
      "Iteration 1505, loss = 0.12996943\n",
      "Iteration 1506, loss = 0.12982737\n",
      "Iteration 1507, loss = 0.12968555\n",
      "Iteration 1508, loss = 0.12954397\n",
      "Iteration 1509, loss = 0.12940263\n",
      "Iteration 1510, loss = 0.12926152\n",
      "Iteration 1511, loss = 0.12912064\n",
      "Iteration 1512, loss = 0.12898000\n",
      "Iteration 1513, loss = 0.12883960\n",
      "Iteration 1514, loss = 0.12869943\n",
      "Iteration 1515, loss = 0.12855950\n",
      "Iteration 1516, loss = 0.12841979\n",
      "Iteration 1517, loss = 0.12828032\n",
      "Iteration 1518, loss = 0.12814109\n",
      "Iteration 1519, loss = 0.12800208\n",
      "Iteration 1520, loss = 0.12786331\n",
      "Iteration 1521, loss = 0.12772477\n",
      "Iteration 1522, loss = 0.12758645\n",
      "Iteration 1523, loss = 0.12744837\n",
      "Iteration 1524, loss = 0.12731052\n",
      "Iteration 1525, loss = 0.12717290\n",
      "Iteration 1526, loss = 0.12703550\n",
      "Iteration 1527, loss = 0.12689834\n",
      "Iteration 1528, loss = 0.12676140\n",
      "Iteration 1529, loss = 0.12662469\n",
      "Iteration 1530, loss = 0.12648820\n",
      "Iteration 1531, loss = 0.12635194\n",
      "Iteration 1532, loss = 0.12621591\n",
      "Iteration 1533, loss = 0.12608010\n",
      "Iteration 1534, loss = 0.12594452\n",
      "Iteration 1535, loss = 0.12580916\n",
      "Iteration 1536, loss = 0.12567403\n",
      "Iteration 1537, loss = 0.12553912\n",
      "Iteration 1538, loss = 0.12540443\n",
      "Iteration 1539, loss = 0.12526997\n",
      "Iteration 1540, loss = 0.12513572\n",
      "Iteration 1541, loss = 0.12500170\n",
      "Iteration 1542, loss = 0.12486790\n",
      "Iteration 1543, loss = 0.12473433\n",
      "Iteration 1544, loss = 0.12460097\n",
      "Iteration 1545, loss = 0.12446783\n",
      "Iteration 1546, loss = 0.12433491\n",
      "Iteration 1547, loss = 0.12420221\n",
      "Iteration 1548, loss = 0.12406973\n",
      "Iteration 1549, loss = 0.12393747\n",
      "Iteration 1550, loss = 0.12380542\n",
      "Iteration 1551, loss = 0.12367359\n",
      "Iteration 1552, loss = 0.12354198\n",
      "Iteration 1553, loss = 0.12341059\n",
      "Iteration 1554, loss = 0.12327941\n",
      "Iteration 1555, loss = 0.12314845\n",
      "Iteration 1556, loss = 0.12301770\n",
      "Iteration 1557, loss = 0.12288716\n",
      "Iteration 1558, loss = 0.12275684\n",
      "Iteration 1559, loss = 0.12262674\n",
      "Iteration 1560, loss = 0.12249685\n",
      "Iteration 1561, loss = 0.12236717\n",
      "Iteration 1562, loss = 0.12223770\n",
      "Iteration 1563, loss = 0.12210845\n",
      "Iteration 1564, loss = 0.12197940\n",
      "Iteration 1565, loss = 0.12185057\n",
      "Iteration 1566, loss = 0.12172195\n",
      "Iteration 1567, loss = 0.12159354\n",
      "Iteration 1568, loss = 0.12146534\n",
      "Iteration 1569, loss = 0.12133734\n",
      "Iteration 1570, loss = 0.12120956\n",
      "Iteration 1571, loss = 0.12108199\n",
      "Iteration 1572, loss = 0.12095462\n",
      "Iteration 1573, loss = 0.12082746\n",
      "Iteration 1574, loss = 0.12070051\n",
      "Iteration 1575, loss = 0.12057377\n",
      "Iteration 1576, loss = 0.12044723\n",
      "Iteration 1577, loss = 0.12032090\n",
      "Iteration 1578, loss = 0.12019478\n",
      "Iteration 1579, loss = 0.12006886\n",
      "Iteration 1580, loss = 0.11994314\n",
      "Iteration 1581, loss = 0.11981763\n",
      "Iteration 1582, loss = 0.11969232\n",
      "Iteration 1583, loss = 0.11956722\n",
      "Iteration 1584, loss = 0.11944232\n",
      "Iteration 1585, loss = 0.11931762\n",
      "Iteration 1586, loss = 0.11919313\n",
      "Iteration 1587, loss = 0.11906883\n",
      "Iteration 1588, loss = 0.11894474\n",
      "Iteration 1589, loss = 0.11882085\n",
      "Iteration 1590, loss = 0.11869716\n",
      "Iteration 1591, loss = 0.11857368\n",
      "Iteration 1592, loss = 0.11845039\n",
      "Iteration 1593, loss = 0.11832730\n",
      "Iteration 1594, loss = 0.11820441\n",
      "Iteration 1595, loss = 0.11808171\n",
      "Iteration 1596, loss = 0.11795922\n",
      "Iteration 1597, loss = 0.11783693\n",
      "Iteration 1598, loss = 0.11771483\n",
      "Iteration 1599, loss = 0.11759293\n",
      "Iteration 1600, loss = 0.11747122\n",
      "Iteration 1601, loss = 0.11734972\n",
      "Iteration 1602, loss = 0.11722841\n",
      "Iteration 1603, loss = 0.11710729\n",
      "Iteration 1604, loss = 0.11698637\n",
      "Iteration 1605, loss = 0.11686564\n",
      "Iteration 1606, loss = 0.11674511\n",
      "Iteration 1607, loss = 0.11662477\n",
      "Iteration 1608, loss = 0.11650463\n",
      "Iteration 1609, loss = 0.11638468\n",
      "Iteration 1610, loss = 0.11626492\n",
      "Iteration 1611, loss = 0.11614536\n",
      "Iteration 1612, loss = 0.11602598\n",
      "Iteration 1613, loss = 0.11590680\n",
      "Iteration 1614, loss = 0.11578781\n",
      "Iteration 1615, loss = 0.11566901\n",
      "Iteration 1616, loss = 0.11555040\n",
      "Iteration 1617, loss = 0.11543199\n",
      "Iteration 1618, loss = 0.11531376\n",
      "Iteration 1619, loss = 0.11519572\n",
      "Iteration 1620, loss = 0.11507787\n",
      "Iteration 1621, loss = 0.11496021\n",
      "Iteration 1622, loss = 0.11484273\n",
      "Iteration 1623, loss = 0.11472545\n",
      "Iteration 1624, loss = 0.11460835\n",
      "Iteration 1625, loss = 0.11449144\n",
      "Iteration 1626, loss = 0.11437471\n",
      "Iteration 1627, loss = 0.11425818\n",
      "Iteration 1628, loss = 0.11414183\n",
      "Iteration 1629, loss = 0.11402566\n",
      "Iteration 1630, loss = 0.11390968\n",
      "Iteration 1631, loss = 0.11379388\n",
      "Iteration 1632, loss = 0.11367827\n",
      "Iteration 1633, loss = 0.11356285\n",
      "Iteration 1634, loss = 0.11344760\n",
      "Iteration 1635, loss = 0.11333254\n",
      "Iteration 1636, loss = 0.11321767\n",
      "Iteration 1637, loss = 0.11310297\n",
      "Iteration 1638, loss = 0.11298846\n",
      "Iteration 1639, loss = 0.11287413\n",
      "Iteration 1640, loss = 0.11275998\n",
      "Iteration 1641, loss = 0.11264602\n",
      "Iteration 1642, loss = 0.11253223\n",
      "Iteration 1643, loss = 0.11241863\n",
      "Iteration 1644, loss = 0.11230520\n",
      "Iteration 1645, loss = 0.11219196\n",
      "Iteration 1646, loss = 0.11207889\n",
      "Iteration 1647, loss = 0.11196600\n",
      "Iteration 1648, loss = 0.11185330\n",
      "Iteration 1649, loss = 0.11174077\n",
      "Iteration 1650, loss = 0.11162842\n",
      "Iteration 1651, loss = 0.11151624\n",
      "Iteration 1652, loss = 0.11140425\n",
      "Iteration 1653, loss = 0.11129243\n",
      "Iteration 1654, loss = 0.11118078\n",
      "Iteration 1655, loss = 0.11106932\n",
      "Iteration 1656, loss = 0.11095803\n",
      "Iteration 1657, loss = 0.11084691\n",
      "Iteration 1658, loss = 0.11073597\n",
      "Iteration 1659, loss = 0.11062521\n",
      "Iteration 1660, loss = 0.11051462\n",
      "Iteration 1661, loss = 0.11040420\n",
      "Iteration 1662, loss = 0.11029396\n",
      "Iteration 1663, loss = 0.11018389\n",
      "Iteration 1664, loss = 0.11007400\n",
      "Iteration 1665, loss = 0.10996428\n",
      "Iteration 1666, loss = 0.10985473\n",
      "Iteration 1667, loss = 0.10974535\n",
      "Iteration 1668, loss = 0.10963614\n",
      "Iteration 1669, loss = 0.10952711\n",
      "Iteration 1670, loss = 0.10941825\n",
      "Iteration 1671, loss = 0.10930955\n",
      "Iteration 1672, loss = 0.10920103\n",
      "Iteration 1673, loss = 0.10909268\n",
      "Iteration 1674, loss = 0.10898450\n",
      "Iteration 1675, loss = 0.10887649\n",
      "Iteration 1676, loss = 0.10876864\n",
      "Iteration 1677, loss = 0.10866097\n",
      "Iteration 1678, loss = 0.10855346\n",
      "Iteration 1679, loss = 0.10844613\n",
      "Iteration 1680, loss = 0.10833896\n",
      "Iteration 1681, loss = 0.10823195\n",
      "Iteration 1682, loss = 0.10812512\n",
      "Iteration 1683, loss = 0.10801845\n",
      "Iteration 1684, loss = 0.10791195\n",
      "Iteration 1685, loss = 0.10780561\n",
      "Iteration 1686, loss = 0.10769944\n",
      "Iteration 1687, loss = 0.10759344\n",
      "Iteration 1688, loss = 0.10748760\n",
      "Iteration 1689, loss = 0.10738193\n",
      "Iteration 1690, loss = 0.10727642\n",
      "Iteration 1691, loss = 0.10717107\n",
      "Iteration 1692, loss = 0.10706589\n",
      "Iteration 1693, loss = 0.10696087\n",
      "Iteration 1694, loss = 0.10685602\n",
      "Iteration 1695, loss = 0.10675133\n",
      "Iteration 1696, loss = 0.10664680\n",
      "Iteration 1697, loss = 0.10654243\n",
      "Iteration 1698, loss = 0.10643823\n",
      "Iteration 1699, loss = 0.10633418\n",
      "Iteration 1700, loss = 0.10623030\n",
      "Iteration 1701, loss = 0.10612658\n",
      "Iteration 1702, loss = 0.10602302\n",
      "Iteration 1703, loss = 0.10591962\n",
      "Iteration 1704, loss = 0.10581638\n",
      "Iteration 1705, loss = 0.10571330\n",
      "Iteration 1706, loss = 0.10561038\n",
      "Iteration 1707, loss = 0.10550762\n",
      "Iteration 1708, loss = 0.10540502\n",
      "Iteration 1709, loss = 0.10530258\n",
      "Iteration 1710, loss = 0.10520029\n",
      "Iteration 1711, loss = 0.10509816\n",
      "Iteration 1712, loss = 0.10499619\n",
      "Iteration 1713, loss = 0.10489438\n",
      "Iteration 1714, loss = 0.10479273\n",
      "Iteration 1715, loss = 0.10469123\n",
      "Iteration 1716, loss = 0.10458989\n",
      "Iteration 1717, loss = 0.10448870\n",
      "Iteration 1718, loss = 0.10438767\n",
      "Iteration 1719, loss = 0.10428680\n",
      "Iteration 1720, loss = 0.10418608\n",
      "Iteration 1721, loss = 0.10408551\n",
      "Iteration 1722, loss = 0.10398510\n",
      "Iteration 1723, loss = 0.10388485\n",
      "Iteration 1724, loss = 0.10378474\n",
      "Iteration 1725, loss = 0.10368480\n",
      "Iteration 1726, loss = 0.10358500\n",
      "Iteration 1727, loss = 0.10348536\n",
      "Iteration 1728, loss = 0.10338587\n",
      "Iteration 1729, loss = 0.10328654\n",
      "Iteration 1730, loss = 0.10318735\n",
      "Iteration 1731, loss = 0.10308832\n",
      "Iteration 1732, loss = 0.10298944\n",
      "Iteration 1733, loss = 0.10289071\n",
      "Iteration 1734, loss = 0.10279213\n",
      "Iteration 1735, loss = 0.10269371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-23 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-23 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-23 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-23 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-23 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-23 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-23 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-23 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-23 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-23 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.001, batch_size=4,\n",
       "              hidden_layer_sizes=4, max_iter=5000, random_state=142,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.001, batch_size=4,\n",
       "              hidden_layer_sizes=4, max_iter=5000, random_state=142,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, batch_size=4,\n",
       "              hidden_layer_sizes=4, max_iter=5000, random_state=142,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(activation='tanh', max_iter=5000, hidden_layer_sizes=(4), batch_size=4, alpha=0.001, solver='adam', random_state=142, verbose=True)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73e8a26e-fa21-4cd1-9463-491453544c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected: [0. 1. 1. 0.]\n",
      "predicted: [0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('expected:', y)\n",
    "print('predicted:', model.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
